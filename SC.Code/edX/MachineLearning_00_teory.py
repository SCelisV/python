# MachineLearning_00_teory.py
# MachineLearning_00_code.py
""" 
# CÃ³digo del cÃ¡lculos de los mÃ³delos:
    - regresiÃ³n Äºineal, 
    - regresiÃ³n mÃºltiple, 
    - regresiÃ³n polinomial, 
    - MÃ¡quinas de Soporte Vectorial - Support Vector Regression
    - Ãrboles de decisiÃ³n
    
# BostonDataSet
# =============================================================================
""" 

# MÃ³delos predictivos con Machine Learning - MPML - SCProjects/0_SCProjects_github.com_SCelisV/python/SC.Code/jupyter/MPML
""" Analizar datos - O-J-O-

    Revisar la descripciÃ³n de los datos, utilizando el mÃ©todo describe() like summary on RStudio.
    Utilizar la media y la mediana para determinar la tendencia central.
    Tener especial atenciÃ³n a los percentiles para poder identificar los rangos de los datos.
    Usar una matriz de correlaciÃ³n para identificar las relaciones fuertes en los datos.
    Hacer visualizaciones para mejorar tu entendimiento de los datos (Box plot, Density plot, Scatter plot).
    Limpiar tus datos (NA, outliers). """

# https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html - include in anaconda3


"""     

    Predecir una clase, tenemos un problema de clasificaciÃ³n.
Applications: Spam detection, image recognition.
Algorithms: SVM, nearest neighbors, random forest, 

    Predecir un valor numÃ©rico, tenemos un problema de regresiÃ³n.
Applications: Drug response, Stock prices.
Algorithms: SVR, nearest neighbors, random forest, 

    Separar los datos en diferentes grupos, tenemos un problema de agrupamiento.
Applications: Customer segmentation, Grouping experiment outcomes
Algorithms: k-Means, spectral clustering, mean-shift,

    Si queremos simplificar la informaciÃ³n, tenemos un problema de reducciÃ³n de dimensiones (reducir la dimensiÃ³n del problema)
 
"""

# Machine learning => automatiza la construcciÃ³n de un mÃ³delo analÃ­tico, uso de algoritmos que aprende de los datos.

# Algoritmos => Supervisado, NO Supervisado, De Refuerzo

# =============================================================================
"""
# 1. -> RegresiÃ³n lÃ­neal - Supervisado - AproximaciÃ³n que modela una relaciÃ³n entre una variable escalar dependiente ("Y") y una o mÃ¡s variables explicativas/independientes ("X")
"""
# DibujarÃ¡ una recta y=mx+b => que nos indicarÃ¡ la tendencia del conjunto de datos, y nos ayudarÃ¡ a predecir en funciÃ³n de un valor X un valor Y. 
# EcuaciÃ³n matemÃ¡tica que representa una lÃ­nea recta. 
# IntentarÃ¡ pasar por la mayor cantidad de datos posibles.
# Es la representaciÃ³n mas adecuada de la distribuciÃ³n de los datos en un diagrama de dispersiÃ³n
# y => coordenada Y
# m => pendiente
# x => coordenada X
# b => Ordenada en el Origen, intercepciÃ³n, el punto Y donde la lÃ­nea cruza el eje Y, relaciÃ³n X,Y

# Procedimiento manual:
# El procedimiento para generar la regresiÃ³n lÃ­neal utilizando la tÃ©cnica de ajuste de mÃ­nimos cuadrados:
    
# - calcular la media aritmÃ©tica y la varianza tanto de los valores de _x_ como los valores de _y_. 

# Por lo que construiremos una tabla donde calculamos los valores cuadrados de ambas variables y calculamos la sumatoria de cada columna.
# formulas: img/RegLin_Formulas.png

# sumatoria x =>
# sumatoria y =>
# sumatoria x*y => 
# sumatoria xÂ² =>
# sumatoria yÂ² =>
#
# y=-0.8926x+31.9160 cambiando el valor de x y pintandola en el plano obtenemos la recta de puntos.

# al unir las graficas de la recta de puntos y los datos originales, 
# se puede observar que tienen comportamientos similares
# por lo tanto por medio de la ecuaciÃ³n podemos predecir el comportamiento de otros datos.

"""
# 2. -> RegresiÃ³n MÃºltiple - Supervisado - Utilizamos regresiÃ³n mÃºltiple cuando estudiamos la posible relaciÃ³n entre varias variables independientes (predictoras o explicativas) y otra variable dependiente (criterio, explicada, respuesta). ... Las modelos de regresiÃ³n nos informan de la presencia de relaciones, pero no del mecanismo causal.
"""
# Multiples variables independientes en el mismo mÃ³delo.
# Importante saber elegir las variables independientes
# =============================================================================
"""
# 3. -> RegresiÃ³n polinomial - Supervisado - 
"""
# Es parecida a la regresiÃ³n lÃ­neal sÃ³lo que extiende la funciÃ³n a un polinomio flexible que se puede curvar si es necesario.. etc. 
# Representa una curva, y graficarÃ¡ el polinomio que mÃ¡s se parezca a las caracterÃ­sticas de los datos.
# Este modelo aÃ±ade curvatura al elevar la variable _x_ a diferentes potencias. De esta manera, se pueden conseguir diferentes funciones que representan y se ajustan mÃ¡s a la distribuciÃ³n de los datos.
# El procedimiento para generar la regresiÃ³n cuadrÃ¡tica utilizando la tÃ©cnica de ajuste de mÃ­nimos cuadrados inicia construyendo un sistema de ecuaciones que son producto de analizar la suma de los cuadrados de los residuos 
# Parabola = Polinomio de grado 2 = XÂ² = potencia mÃ¡s grande - Derivadas parciales - 
# 
# =============================================================================
"""
# 4. -> MÃ¡quinas de Soporte Vectorial -> SVM -> SVR - Supervisado
"""
# Buscan encontrar un hiperplano que separe los puntos compuestos en una categoria de otra. 
# Delimitar con notoriedad cada una de las caracterÃ­sticas de los diferentes conjuntos.
# Las mÃ¡quinas de soporte vectorial buscan encontrar un hiperplano que separe de forma Ã³ptima a los puntos que componen diferentes categorÃ­as unos de los otros. 
# Este tipo de algoritmo suele utilizarse para predecir a que categorÃ­a pertencerÃ¡ un nuevo punto del que no se tenÃ­a informaciÃ³n con anterioridad.
# SVR -> RegresiÃ³n de Soporte Vectorial - Supervisado - SVR
# Support Vector Regression es un algoritmo de regresiÃ³n basado en las mÃ¡quinas de soporte vectorial utilizados para clasificar elementos de diferentes conjuntos. 
# En la siguiente secciÃ³n se explicara cÃ³mo funciona el algoritmo SVR con datos lineales, 
# pero es importante saber que este algoritmo tambiÃ©n funciona para datos no lineales.

# Objetivos: MÃ¡ximizar el margen(las bandas de soporte que rodean a la lÃ­nea de regresiÃ³n) para abarcar la mayorÃ­a de los puntos de nuestro mÃ³delo y Minimizar el error

#     https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR
#     https://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use
# 
# =============================================================================
"""
# 5. -> Ãrboles de decisiÃ³n : NO Supervisado
"""
# https://web.fdi.ucm.es/posgrado/conferencias/JorgeMartin-slides.pdf
# CART - Ãrboles de clasificaciÃ³n (asigna una etiqueta) y regresiÃ³n (asigna una valor) 
# pretenden explicar o predecir una variable a partir de un conjunto de variables predictoras utilizando un conjunto de reglas sencillas. 
# Se hacen diferentes intervalos y a cada uno se asigna un valor de tal forma se podrÃ¡ identificar cada punto a que rango pertenece
# El procedimiento para generar un Ã¡rbol de regresiÃ³n empieza partiendo la totalidad del intervalo de los datos de la variable independiente x en diferentes intervalos pequeÃ±os. En nuestro ejemplo aprovecharemos que es posible identificar que los datos parecen estar agrupados en los siguientes cuatro intervalos:
    
# =============================================================================
#     ğ¼ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘£ğ‘ğ‘™ğ‘œ1: 0 â‰¤ ğ‘¥ < 0.25 => ğ‘¦=10
#     ğ¼ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘£ğ‘ğ‘™ğ‘œ2: 0.25 â‰¤ ğ‘¥ < 0.5 => ğ‘¦=15
#     ğ¼ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘£ğ‘ğ‘™ğ‘œ3: 0.5 â‰¤ ğ‘¥ < 0.75 => ğ‘¦=20
#     ğ¼ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘£ğ‘ğ‘™ğ‘œ4: 0.75 â‰¤ ğ‘¥ â‰¤ 1.0 => ğ‘¦=15
#    
#    Asignarle un valor numÃ©rico a una entrada x de la cuÃ¡l no se conocÃ­a su valor de salida y previamente.
# =============================================================================


"""
# 6. -> Redes neuronales => recibir, procesar y transmitir informaciÃ³n
"""
# PerceptrÃ³n => neurona artificial, la uniÃ³n de varios crean una red neuronal artificial. <=> simulan a las humanas y son capaces de transmitir seÃ±ales entre ellas e ir modificando las entradas de las neuronas para obtener valores de salida
# Aprenden a partir de la experiencia para predecir

# Se compone de:
# - Canales/SeÃ±ales de entrada - Dentritas
# - FunciÃ³n de activaciÃ³n - Soma o nÃºcleo - (uniÃ³n sumadora)
# - Canal de salida y AxÃ³n.

"""  n
 Î£ WiXi + b
i=0  """


# Tipos de aprendizaje => Algoritmos => Supervisado, NO Supervisado, De Refuerzo

""" Aprendizaje supervisado => proporcionamos datos de test(conocidos)
Los algoritmos de aprendizaje supervisado son aquellos que trabajan primeramente aprendiendo con un conjunto de datos de entrenamiento "etiquetados", los cuales recibe para posteriormente trabajar con ellos, intentando asignarles correctamente una etiqueta que coincida con la que previamente tenÃ­a. """

""" El comportamiento del algoritmo se corrige en medida a cuÃ¡ntas veces fallo al momento de etiquetar los datos de prueba y posteriormente modifica su comportamiento para las prÃ³ximas ejecuciones. DespuÃ©s de terminar de entrenar con los datos etiquetados, se somete al algoritmo a trabajar con un conjunto de datos nuevos para los que no se conoce la etiqueta. 
 """
# Supervisado => necesita datos previamente etiquetados(lo que es correcto y lo que no es correcto) para aprender a realizar el trabajo. En base a esto, el algoritmo aprenderÃ¡ a resolver problemas futuros similares.a

# -> RegresiÃ³n lÃ­neal,
# -> RegresiÃ³n polinomial,
# -> MÃ¡quinas de Soporte Vectorial -> SVM - SVR

""" Aprendizaje no supervisado

Los algoritmos de aprendizaje no supervisado no cuentan con un conjunto de datos "etiquetados" con los cuales puede entrenar y buscan intentar encontrar algÃºn tipo de organizaciÃ³n o patrÃ³n en los datos de entrada que recibe. Estos algoritmos suelen tener un comportamiento "exploratorio", y en el caso de enfrentar un problema de agrupamiento, estos algoritmos intentan agrupar a los datos por medio de caracterÃ­sticas similares pero no sin saber previamente que tipos de datos va a agrupar. 
 """
 
# NO Supervisado => necesita indicaciones previas, No necesita datos previamente etiquetados. Aprende a comprender y a analizar la informaciÃ³n. PrÃ¡ctica sobre los datos que tiene. 
# Busca relaciones entre los datos - para identificarlos - pero no los podrÃ¡ "etiquetar"

# -> K vecinos mÃ¡s cercanos - KNN
# -> Ãrboles de decisiÃ³n :  https://web.fdi.ucm.es/posgrado/conferencias/JorgeMartin-slides.pdf
#   CART - Ãrboles de clasificaciÃ³n y regresiÃ³n 
# pretenden explicar o predecir una variable a partir de un conjunto de variables predictoras utilizando un conjunto de reglas sencillas. 
# -> Random Forest -> Bosques Aleatorios
# -> k-medias -> no supervisado, usado para clusterizaciÃ³n

""" 

Aprendizaje reforzado

Las tÃ©cnicas de aprendizaje por refuerzo se basan en modificar la respuesta del algoritmo utilizando un proceso de retroalimentaciÃ³n basado en un conjunto de recompensas y castigos que le permiten al algoritmo identificar cuando alguna de las acciones que realizo previamente obtuvo buenos resultados o fue un comportamiento que deberÃ¡ evitar en futuras ejecuciones. Estas tÃ©cnicas de aprendizaje intentan simular el proceso de aprendizaje humano, simulando la sensaciÃ³n de que el algoritmo aprende obteniendo informaciÃ³n de cÃ³mo se modifica el mundo que lo rodea en respuesta de las acciones que produce.  """

# De Refuerzo => aprende por su cuenta, en base a conocimientos introducidos previamente, aprende en funciÃ³n del Ã©xito Ã³ fracaso.
# La acciÃ³n le puede generar una recompensa y la repites.. si es un castigo lo evitas 


# scikit-learn.org/ -- Machine Learning en Python 
# https://scikit-learn.org/stable
""" 
https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets

https://scikit-learn.org/stable/datasets/index.html#boston-dataset
 """





# limpieza de datos
# verificar los null's


# si NULOS => FALSE => NO hay null's

# calculamos el coeficiente de CORrelaciÃ³n
# NO Diagonales => CorrelaciÃ³n â€”> entran escalados entre -1 y 1.
# Si las variables estÃ¡n muy correlacionadas R tendrÃ¡ valor entre -1 y 1.
# Existen dos tipos de correlaciones tanto negativas como positivas
# CorrelaciÃ³n positiva => > 0 , aumenta una variable por tanto aumenta la otra.
# ex: Estatura - Peso - crecen a la vez.
# CorrelaciÃ³n negativa => < 0, aumenta una variable y disminuye la otra.
# ex: Velocidad vs AutonomÃ­a. > aceleraciÃ³n < autonomÃ­a

# Cuando estÃ¡n mÃ¡s cercanos a 1, la nube de puntos va a ser mÃ¡s perfecta a una lÃ­nea.
# si hay alguna correlaciÃ³n que me dÃ© 1.00000000 podemos decir que es una
# correlaciÃ³n perfecta., Ã³ que esas dos variables son la misma variable, independiente de las
# unidades en que este medida.
# correlaciÃ³n baja estÃ¡ entre -0,2 y +0,2
# Valores altos => cercano a -1 Ã³ +1 â€”> en la primera fila y primera columna.
# Valores bajos => cercanos a 0 => resto de elementos entre la segunda fila y la diagonal.
# R: = 0 => Indica que no tienen nada de correlaciÃ³n.
# R: = 1 => CorrelaciÃ³n Perfecta.

# R makes it easy to combine multiple plots into one overall graph, using either the
# par( ) or layout( ) function.

# With the par( ) function, you can include the option mfrow=c(nrows, ncols) to create a matrix of nrows x ncols plots that are filled in by row. mfcol=c(nrows, ncols) fills in the matrix by columns.


# - Cual es la probabilidad de un evento mas desfavorable.
# p-value -> indica si el regresor influye Ã³ no influye en la variable de respuesta.
# p-value: < 2.2e-16 si < 0.05 => indica si el regresor influye , es decir, influye con un 95%.
# Si p-value: < 0.05 Influye
# Si p-value > 0.05 No tenemos evidencia para decir que influye significativamente.


# Una recta de regresiÃ³n debe cumplir:
# Que la distancia entre los puntos a la recta de regresiÃ³n sea mÃ­nima.

# HipÃ³tesis de Partida - Diagnosis
# - Linealidad -> Se DA por supuesta - Las variables siguen una relaciÃ³n lineal.
# - Normalidad -> Se debe comprobar que los residuos siguen una distribuciÃ³n normal
#     La Y para TODAS las X NO sigue una distribuciÃ³n normal.. PERO
#     La variable Y para un determinado valor de X SI sigue una distribuciÃ³n normal.
# - Homocedasticidad -> varianza constante Ã³ variabilidad constante - distribuciÃ³n de probabilidad de idÃ©ntica amplitud para cada variable aleatoria -
    # La nube de puntos tenga un grosor constante.
    # Heterocedastico -> varianza NO es constante Ã³ variabilidad NO constante, a pesar de que la nube de puntos tiene forma lineal la varianza Ã³ dispersiÃ³n va aumentando:
# - Independencia -> Cuando las observaciones en sÃ­ no estÃ¡n relacionadas.



# R-squared: # Somos capaces de explicar el 77% de la variaciÃ³n de G3.

# 2. -> RegresiÃ³n logÃ­stica - Supervisado - Predecir el resultado de una variable categorica en funciÃ³n de otras independientes.
# Modela la probabilidad de que un evento ocurra en funciÃ³n de otros factores, mÃ©todo de clasificaciÃ³n (binaria, 1 Ã³ 0).
# DibujarÃ¡ una curva que nos indicarÃ¡ la tendencia del conjunto de datos, y nos ayudarÃ¡ a predecir en funciÃ³n de un valor X un valor Y.
# Siempre serÃ¡ entre 0 Ã³ 1

# Si resultado >= 0.5 => devuelve 1
# Si resultado < 0.5 => devuelve 0


# Matriz de ConfusiÃ³n => Compara el valor real con el valor de prediccion

# Real vs predicciÃ³n

# SI vs SI => PC => Positivo Correcto
# NO vs NO => NC => Negativo Correcto
# SI vs NO => FP => Falsos Positivos => Error Tipo 1
# NO vs SI => FN => Falsos Negativos => Error Tipo 2

# La precisiÃ³n sirve para saber la probabilidad de acierto en la predicciÃ³n => (PC + NC ) / Total
# La tasa de error sirve para saber la probabilidad de error en la predicciÃ³n => (FP + FN) / Total

# API Command => kaggle kernels pull alexisbcook/titanic-tutorial

# Para limpiar estos datos lo que haremos serÃ¡ sustituir los valores NA de las edades por la media de edad en cada Pclass

# One Way
# creamos una funciÃ³n que reemplace los valores nulls del dataSet por 0, le pasamos la columna edad, y la columna clase
# =============================================================================
# nulos <- function(Age, Pclass) {
#     newAge <- Age
#     for(i in 1:length(Age)){
#         if( is.na(Age[i]) ) { # si es null
#             if(Pclass[i] == 1){
#                 newAge[i] <- 33.00
#             } else if(Pclass[i] == 2){
#                 newAge[i] <- 28.00
#             } else if(Pclass[i] == 3){
#                 newAge[i] <- 18.00
#             }   
#         }        
#     }
#     return(newAge)
# }
# =============================================================================

# =============================================================================
# # Funciones con varios argumentos y aplicar el resultado a una columna
# datos.19$Age <- nulos(datos.19$Age, datos.19$Pclass)
# 
# 
# # Two Way
# # creamos una funciÃ³n que reemplace los valores nulls del dataSet por 0, para poder calcular la media
# 
# factor(datos.19$Pclass)
# 
# nulos <- function(x) {
#     #print(x)
#     if( is.na(x) )
#         {return (0)}
#     else
#         {return (x)}
# }
# 
# =============================================================================


# Three Way - Otra forma de codificar la funciÃ³n..
# creamos una funciÃ³n que reciba la edad y la clase, reemplaza los valores nulls del dataSet por la media que corresponda con la clase:
# =============================================================================
# nulos <- function(x, y) {
#     # x => edad => datos.19$Age
#     # y => class => datos.19$Pclass
#     print(x)
#     print(y)
#     if (y == 1){
#         if( is.na(x) )
#             {return (33)}
#         else
#             {return (x)}
#     } else if (y == 2) {
#             if( is.na(x) )
#                 {return (28)}
#             else
#                 {return (x)}
#     } else if (y == 3) {
#             if( is.na(x) )
#                 {return (18)}
#             else
#                 {return (x)}
#     }
#     else {return (x)}
# }
# 
# =============================================================================


# 1. -> K vecinos mÃ¡s cercanos - KNN => clasificaciÃ³n, estima la probabilidad de que un elemento "x" pertenezca a una clase "C", a partir de la informaciÃ³n proporcionada.

# 2. -> Ãrboles de decisiÃ³n :  https://web.fdi.ucm.es/posgrado/conferencias/JorgeMartin-slides.pdf
# Sirven Representar y Categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluciÃ³n de un problema.

# 3. -> Random Forest -> Bosques Aleatorios - Algoritmo de clasificaciÃ³n
# combinaciÃ³n entre arboles de decisiÃ³n en la que cada arbol selecciona una clase y luego se combinan las decisiones de cada uno para seleccionar la mejor opciÃ³n.
# Maneja cientos de variables de entrada, eficiente en DB grandes

# 3. -> MÃ¡quinas de Soporte Vectorial -> SVM -> Conjunto de algoritmos de aprendizaje supervisado para resolver problemas de clasificaciÃ³n y regresiÃ³n.
# Analizar datos y resolver patrones
# Dado un conjunto de ejemplos de entrenamiento, podemos etiquetar las clases y entrenar una SVM para construir un mÃ³delo que prediga la clase de una nueva muestra.

# Se separan las clases en dos espacios lo mÃ¡s amplio posible..


# 4. -> k-medias -> no supervisado, usado para clusterizaciÃ³n
# ParticiÃ³n de un conjunto de "n" observaciones en "k" grupos, en el que cada observaciÃ³n pertenece al grupo cuyo valor medio es mÃ¡s cercano. - mineria de datos

# Cross Validation -O-J-O-
# 

